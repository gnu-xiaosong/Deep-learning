{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X, b, w):\n",
    "  \"\"\"Calculate the sigmoid function.\n",
    "\n",
    "  Args:\n",
    "    X: MxK ndarray of the input features. M: number of samples, K: number of features.\n",
    "    b: bias.\n",
    "    w: weights. ndarray of shape (K,).\n",
    "\n",
    "  Returns:\n",
    "    Sigmoid function value.\n",
    "  \"\"\"\n",
    "\n",
    "  z = b + X @ w\n",
    "  return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def logloss(y, X, b, w):\n",
    "  \"\"\"Calculate the log-loss.\n",
    "\n",
    "  Args:\n",
    "    y: 1D ndarray of the target variable.\n",
    "    X: MxK ndarray of the input features. M: number of samples, K: number of features.\n",
    "    b: bias.\n",
    "    w: weights. ndarray of shape (K,).\n",
    "\n",
    "  Returns:\n",
    "    Log-loss.\n",
    "  \"\"\"\n",
    "\n",
    "  y_hat = sigmoid(X, b, w)\n",
    "  # When y_hat is almost equal to 0 or 1, log(y_hat) or log(1-y_hat) returns -inf.\n",
    "  # To prevent this, subtract or add an infinitesimal value (epsilon) to y_hat\n",
    "  # when it is close to 0 or 1.\n",
    "  epsilon = 1.e-7\n",
    "  y_hat = np.where(y_hat > 1 - epsilon, y_hat - epsilon, y_hat)\n",
    "  y_hat = np.where(y_hat < epsilon, y_hat + epsilon, y_hat)\n",
    "\n",
    "  loss = y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat)\n",
    "  return loss\n",
    "\n",
    "\n",
    "def costfcn(y, X, b, w):\n",
    "  \"\"\"Calculate the log-loss cost function.\n",
    "\n",
    "  Args:\n",
    "    y: 1D ndarray of the target variable.\n",
    "    X: MxK ndarray of the input features. M: number of samples, K: number of features.\n",
    "    b: bias.\n",
    "    w: weights. ndarray of shape (K,).\n",
    "\n",
    "  Returns:\n",
    "    Cost function value.\n",
    "  \"\"\"\n",
    "\n",
    "  l = logloss(y, X, b, w)\n",
    "  J = -np.mean(l)\n",
    "\n",
    "  return J\n",
    "\n",
    "\n",
    "def gradient(y, X, b, w, lambda_):\n",
    "  \"\"\"Calculate gradients of the cost function with L2 regularization.\n",
    "\n",
    "  Args:\n",
    "    y: 1D ndarray of the target variable.\n",
    "    X: MxK ndarray of the input features. M: number of samples, K: number of features.\n",
    "    b: bias.\n",
    "    w: weights. ndarray of shape (K,).\n",
    "    lambda_: regularization parameter.\n",
    "\n",
    "  Returns:\n",
    "    dJ/db, dJ/dw.\n",
    "  \"\"\"\n",
    "\n",
    "  K = X.shape[1] # number of features.\n",
    "  y_hat = sigmoid(X, b, w)\n",
    "  dJdb = np.mean(y_hat - y)\n",
    "  dJdw = X.T @ (y_hat - y) / len(y) + 2 *lambda_ * w # Regularization term added.\n",
    "\n",
    "  return dJdb, dJdw\n",
    "\n",
    "\n",
    "def gradient_descent(y, X, alpha=0.01, tol=1.e-7, max_iter=10000):\n",
    "  \"\"\"Estimate parameters using a gradient decent algorithm.\n",
    "\n",
    "  Args:\n",
    "    y: 1D ndarray of the target variable.\n",
    "    X: MxK ndarray of the input features. M: number of samples, K: number of features.\n",
    "    alpha: learning rate.\n",
    "    tol: Tolerance. If the relative change of the cost function is less than tol,\n",
    "      the cost function is considered to have converged to a minimum.\n",
    "    max_iter: Maximum iterations.\n",
    "    lambda_: regularization parameter.\n",
    "\n",
    "  Returns:\n",
    "    Parameter estimates, b and w.\n",
    "  \"\"\"\n",
    "\n",
    "  K = X.shape[1] # number of features.\n",
    "  b = np.random.uniform()  # Random initial value of b.\n",
    "  w = np.random.uniform(size=K)  # Random initial value of w.\n",
    "\n",
    "  J0 = costfcn(y, X, b, w)\n",
    "\n",
    "  for i in range(max_iter):\n",
    "    # Update parameters.\n",
    "    dJdb, dJdw = gradient(y, X, b, w)\n",
    "    b -= alpha * dJdb\n",
    "    w -= alpha * dJdw\n",
    "\n",
    "    J = costfcn(y, X, b, w)\n",
    "    if i % 100 == 0:\n",
    "      print(f'{i}: b = {b}, w = {w}, J = {J}')\n",
    "\n",
    "    # Check convergence.\n",
    "    if np.abs((J-J0) / J0) < tol:\n",
    "      break\n",
    "    else:\n",
    "      J0 = J\n",
    "\n",
    "  if i == max_iter:\n",
    "    print('Maximum iteration reached before convergence.')\n",
    "\n",
    "  return b, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of 1s: 0.503\n"
     ]
    }
   ],
   "source": [
    "m = 10000\n",
    "b = 0.5\n",
    "w = [3, 1, -1, 5]\n",
    "X = np.random.uniform(-10, 10, (m, 4))\n",
    "z = b + X @ w + np.random.normal(0, 2, m)\n",
    "s = 1 / (1 + np.exp(-z))\n",
    "y = (s > 0.5).astype(int)\n",
    "print('Proportion of 1s:', y.sum()/m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Regularization parameter (lambda): 0.01\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'gradient_descent_with_regularization' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-6f3aabbe664b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlambda_values\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\\nRegularization parameter (lambda): {lambda_}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mb_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_descent_with_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bias:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'weights:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gradient_descent_with_regularization' is not defined"
     ]
    }
   ],
   "source": [
    "# Testing with different values of lamba\n",
    "lambda_values = [0.01, 0.1, 1.0]  # Different values of lambda (regularization parameter)\n",
    "\n",
    "for lambda_ in lambda_values:\n",
    "    print(f'\\nRegularization parameter (lambda): {lambda_}')\n",
    "    b_hat, w_hat = gradient_descent_with_regularization(y, X, alpha=0.3, lambda_=lambda_, tol=1.e-5, max_iter=10000)\n",
    "    print('bias:', b_hat)\n",
    "    print('weights:', w_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "gradient() missing 1 required positional argument: 'lambda_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-8f21266cd0db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mb_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bias:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'weights:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-f0b84786a37a>\u001b[0m in \u001b[0;36mgradient_descent\u001b[0;34m(y, X, alpha, tol, max_iter)\u001b[0m\n\u001b[1;32m    105\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;31m# Update parameters.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0mdJdb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdJdw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m     \u001b[0mb\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdJdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0mw\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdJdw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: gradient() missing 1 required positional argument: 'lambda_'"
     ]
    }
   ],
   "source": [
    "b_hat, w_hat = gradient_descent(y, X, alpha=0.3, tol=1.e-5, max_iter=10000)\n",
    "print('bias:', b_hat)\n",
    "print('weights:', w_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'b_hat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-1ddc443048df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mz_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb_hat\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mw_hat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mz_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mz_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'b_hat' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = b + X @ w\n",
    "plt.scatter(z, y)\n",
    "z_hat = b_hat + X @ w_hat\n",
    "z_hat = np.sort(z_hat)\n",
    "y_hat = 1 / (1 + np.exp(-z_hat))\n",
    "plt.plot(z_hat, y_hat, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LogisticRegression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-4c1e5ebec181>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bias:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintercept_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'weights:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LogisticRegression' is not defined"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(penalty=None).fit(X, y)\n",
    "print('bias:', clf.intercept_)\n",
    "print('weights:', clf.coef_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
