{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def costfcn_v(y, X, b, w):\n",
    "  \"\"\"Calculate the mean-squared error cost function for a bivariate linear regression.\n",
    "\n",
    "  Args:\n",
    "    y: 1D ndarray of the target variable.\n",
    "    X: 1D ndarray of the input features.\n",
    "    b: bias.\n",
    "    w: weight.\n",
    "\n",
    "  Returns:\n",
    "    Cost function value.\n",
    "  \"\"\"\n",
    "\n",
    "  y_hat = b + w * X\n",
    "  J = np.mean((y_hat - y) ** 2) / 2\n",
    "\n",
    "  return J\n",
    "\n",
    "def gradient_v(y, X, b, w):\n",
    "  \"\"\"Calculate gradients of the cost function.\n",
    "\n",
    "  Args:\n",
    "    y: 1D ndarray of the target variable.\n",
    "    X: 1D ndarray of the input features.\n",
    "    b: bias.\n",
    "    w: weight.\n",
    "\n",
    "  Returns:\n",
    "    dJ/db, dJ/dw.\n",
    "  \"\"\"\n",
    "\n",
    "  y_hat = b + w * X\n",
    "  dJdb = np.mean(y_hat - y)\n",
    "  dJdw = np.mean((y_hat - y) * X)\n",
    "\n",
    "  return dJdb, dJdw\n",
    "\n",
    "def graident_descent_v(y, X, alpha=0.1, tol=1.e-7):\n",
    "  \"\"\"Estimate parameters using a gradient decent algorithm.\n",
    "\n",
    "  Args:\n",
    "    y: 1D ndarray of the target variable.\n",
    "    X: 1D ndarray of the input features.\n",
    "    alpha: learning rate.\n",
    "    tol: Tolerance. If the relative change of the cost function is less than tol,\n",
    "      the cost function is considered to have converged to a minimum.\n",
    "\n",
    "  Returns:\n",
    "    Parameter estimates, b and w.\n",
    "  \"\"\"\n",
    "\n",
    "  max_iter = 10000  # maximum iteration\n",
    "  b = np.random.uniform()  # Random initial value of b.\n",
    "  w = np.random.uniform()  # Random initial value of w.\n",
    "\n",
    "  J0 = costfcn_v(y, X, b, w)\n",
    "\n",
    "  for i in range(max_iter):\n",
    "    # Update parameters.\n",
    "    dJdb, dJdw = gradient_v(y, X, b, w)\n",
    "    b -= alpha * dJdb\n",
    "    w -= alpha * dJdw\n",
    "\n",
    "    J = costfcn_v(y, X, b, w)\n",
    "    print(f'{i}: b = {b}, w = {w}, J = {J}')\n",
    "\n",
    "    # Check convergence.\n",
    "    if np.abs((J-J0) / J0) < tol:\n",
    "      break\n",
    "    else:\n",
    "      J0 = J\n",
    "\n",
    "  if i == max_iter:\n",
    "    print('Maximum iteration reached before convergence.')\n",
    "\n",
    "  return b, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cost_function(X, y, b, w):\n",
    "    m = len(y)\n",
    "    predictions = X.dot(w) + b\n",
    "    cost = (1 / (2 * m)) * np.sum(np.square(predictions - y))\n",
    "    return cost\n",
    "\n",
    "def gradient_descent(X, y, b, w, learning_rate, iterations):\n",
    "    m = len(y)\n",
    "    cost_history = np.zeros(iterations)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        predictions = X.dot(w) + b\n",
    "        errors = predictions - y\n",
    "        w_gradient = (1/m) * X.T.dot(errors)\n",
    "        b_gradient -= learning_rate * gradient\n",
    "        cost_history[i] = cost_function(X, y, b, w)\n",
    "    return b, w, cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data\n",
    "m = 1000\n",
    "b = 0.5\n",
    "w = [3, 1, -0.5, 10]\n",
    "X = np.random.uniform(-10, 10, (m, 4))\n",
    "y = b + X @ w + np. random.normal(0, 2, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column of ones to X for the bias term\n",
    "X_b = np.c_[np.ones((m,1)),X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial parameters\n",
    "theta = np. zeros(5) # Including bias term\n",
    "learning_rate = 0.01\n",
    "iterations = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "gradient_descent() missing 1 required positional argument: 'iterations'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-da327686affa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Run gradient descent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtheta_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Parameter estimates:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Intercept:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta_final\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: gradient_descent() missing 1 required positional argument: 'iterations'"
     ]
    }
   ],
   "source": [
    "# Run gradient descent\n",
    "theta_final, cost_history = gradient_descent(X_b, y, theta, learning_rate, iterations)\n",
    "\n",
    "print(\"Parameter estimates:\")\n",
    "print(\"Intercept:\", theta_final[0])\n",
    "print(\"Coefficients:\", theta_final[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cost_history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-04cce57ac053>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# PLot cost history to visualize convergence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'blue'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Iterations'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cost'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cost_history' is not defined"
     ]
    }
   ],
   "source": [
    "# PLot cost history to visualize convergence\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(1, iterations +1), cost_history, color='blue')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Gradient Descent Convergence')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
